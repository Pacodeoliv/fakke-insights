version: '3.8'

services:
  airflow:
    image: apache/airflow:2.6.3-python3.10
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__FERNET_KEY=46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=true
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth
      - PYTHONPATH=/opt/airflow/src
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-sa-east-1}
    user: "${AIRFLOW_UID:-50000}:0"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
      - ./data:/opt/airflow/data
      - ./src:/opt/airflow/src
      - ./requirements.txt:/opt/airflow/requirements.txt
    command: >
      bash -c "
        mkdir -p /opt/airflow/data &&
        chown -R ${AIRFLOW_UID:-50000}:0 /opt/airflow/data &&
        pip install --user --no-cache-dir -r requirements.txt &&
        airflow db init &&
        airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@example.com --password admin &&
        (airflow webserver & airflow scheduler)
      "
    ports:
      - "8080:8080"

volumes:
  airflow-db:
  airflow-dags:
  airflow-logs:
  airflow-plugins:
  airflow-data:
  airflow-src: 